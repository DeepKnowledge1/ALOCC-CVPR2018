{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepKnowledge1/ALOCC-CVPR2018/blob/master/anomavision_colab_trt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10034f46",
      "metadata": {
        "id": "10034f46"
      },
      "source": [
        "# Anomaly Detection with AnomaVision (Colab Notebook)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc5a027d",
      "metadata": {
        "id": "bc5a027d"
      },
      "source": [
        "This notebook demonstrates how to train, export, and detect anomalies using the provided `train.py`, `export.py`, and `detect.py` scripts within a Google Colab environment. It follows a 'getting started' approach with direct code snippets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "095dc63e",
      "metadata": {
        "id": "095dc63e"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17ed3e05",
      "metadata": {
        "id": "17ed3e05"
      },
      "source": [
        "### 1.1 Check GPU (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3524d8eb",
      "metadata": {
        "id": "3524d8eb"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Check GPU (optional)\n",
        "!nvidia-smi || echo \"No NVIDIA GPU detected (that's okay for a quick CPU demo).\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf AnomaVision\n",
        "!rm -rf *"
      ],
      "metadata": {
        "id": "vxTHL3rbc7c6"
      },
      "id": "vxTHL3rbc7c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e5dc84f7",
      "metadata": {
        "id": "e5dc84f7"
      },
      "source": [
        "### 1.2 Install AnomaVision (from GitHub using Poetry)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27082230",
      "metadata": {
        "id": "27082230"
      },
      "outputs": [],
      "source": [
        "\n",
        "# If running in Colab, this will take a minute.\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install poetry\n",
        "\n",
        "!git clone --depth 1 -b bugfix/mah_dtype https://github.com/DeepKnowledge1/AnomaVision.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AnomaVision\n",
        "\n",
        "!poetry config virtualenvs.create false\n",
        "\n",
        "# install dependencies & project using poetry\n",
        "!poetry install --no-interaction --no-ansi\n",
        "\n",
        "# make sure it's importable in the current Python env\n",
        "import sys, pathlib\n",
        "sys.path.append(str(pathlib.Path(\".\").resolve()))\n",
        "\n",
        "print(\"‚úÖ AnomaVision installed via Poetry.\")\n"
      ],
      "metadata": {
        "id": "MVZp0INox-Id"
      },
      "id": "MVZp0INox-Id",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "import torch, sys, os, time\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"üîç AnomaVision - Optimized PaDiM Anomaly Detection\")\n",
        "print(\"=\" * 60)\n",
        "print(\"=== Hardware Check ===\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ CUDA available! GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version (PyTorch built with): {torch.version.cuda}\")\n",
        "    try:\n",
        "        props = torch.cuda.get_device_properties(0)\n",
        "        print(f\"GPU Memory: {props.total_memory / 1e9:.1f} GB\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è CUDA not available - using CPU\")\n",
        "    device = torch.device('cpu')\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"Selected device: {device}\")"
      ],
      "metadata": {
        "id": "-x_b2brLbDEQ"
      },
      "id": "-x_b2brLbDEQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4 --force-reinstall"
      ],
      "metadata": {
        "id": "c013vzc93P2N"
      },
      "id": "c013vzc93P2N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bbf55715",
      "metadata": {
        "id": "bbf55715"
      },
      "source": [
        "### 1.3 Verify AnomaVision Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bb992498",
      "metadata": {
        "id": "bb992498",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2453e9ad-9838-4b9d-d6b2-4ee43d3b6323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.26.4\n",
            "4.11.0\n",
            "2.8.0+cu128\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(np.__version__)\n",
        "print(cv2.__version__)\n",
        "print(torch.__version__ )\n",
        "\n",
        "import anomavision\n",
        "\n",
        "\n",
        "print(getattr(anomavision, \"version\", None))   # fallback\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73b5992a",
      "metadata": {
        "id": "73b5992a"
      },
      "source": [
        "### 1.4 Upload Provided Scripts (Optional - if you have custom scripts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d363743a",
      "metadata": {
        "id": "d363743a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# @title Upload images from your computer\n",
        "\n",
        "data= torch.randn(10, 3, 224, 224).to(\"cuda\")\n",
        "\n",
        "# from google.colab import files\n",
        "# import os\n",
        "\n",
        "# upload_dir = \"uploaded_images\"\n",
        "# os.makedirs(upload_dir, exist_ok=True)\n",
        "\n",
        "# uploaded = files.upload()\n",
        "# for filename, content in uploaded.items():\n",
        "#     filepath = os.path.join(upload_dir, filename)\n",
        "#     with open(filepath, \"wb\") as f:\n",
        "#         f.write(content)\n",
        "#     print(f\"‚úÖ Saved {filename} to {filepath}\")\n",
        "\n",
        "\n",
        "\n",
        "# upload_dir = \"uploaded_images\"\n",
        "# print(\"Files in uploaded_images/:\")\n",
        "# print(os.listdir(upload_dir))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload images from your computer\n",
        "\n",
        "\n",
        "data2= torch.randn(1, 3, 224, 224).to(\"cuda\")\n",
        "\n",
        "\n",
        "# from google.colab import files\n",
        "# import os\n",
        "\n",
        "# test_dir = \"test_images\"\n",
        "# os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# test = files.upload()\n",
        "# for filename, content in test.items():\n",
        "#     filepath = os.path.join(test_dir, filename)\n",
        "#     with open(filepath, \"wb\") as f:\n",
        "#         f.write(content)\n",
        "#     print(f\"‚úÖ Saved {filename} to {filepath}\")\n",
        "\n",
        "\n",
        "\n",
        "# test_dir = \"test_images\"\n",
        "# print(\"Files in test_images/:\")\n",
        "# print(os.listdir(test_dir))"
      ],
      "metadata": {
        "id": "6VHRz-a8tM40"
      },
      "id": "6VHRz-a8tM40",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload test images from your computer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JZ5Rf7KxtQhU"
      },
      "id": "JZ5Rf7KxtQhU"
    },
    {
      "cell_type": "markdown",
      "id": "2ee5806c",
      "metadata": {
        "id": "2ee5806c"
      },
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2e46150",
      "metadata": {
        "id": "d2e46150"
      },
      "source": [
        "Define parameters for training, export, and detection. These will replace command-line arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "78b57fe9",
      "metadata": {
        "id": "78b57fe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6df1f8b-1a71-492c-ae75-50a7369b0a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration defined. Please review and adjust `dataset_path` and `class_name`.\n"
          ]
        }
      ],
      "source": [
        "from pickle import FALSE\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from easydict import EasyDict as edict\n",
        "\n",
        "# --- Common Configuration ---\n",
        "# This config will be used across train, export, and detect scripts.\n",
        "# Adjust these parameters based on your dataset and model requirements.\n",
        "common_config = edict({\n",
        "    # IMPORTANT: Adjust dataset_path to point to your unzipped dataset within /content/dataset\n",
        "    # For example, if you unzipped 'mvtec_ad.zip' and it created 'mvtec_ad/bottle' inside 'dataset',\n",
        "    # then dataset_path should be './dataset/mvtec_ad'\n",
        "    'dataset_path': \"upload_dir\", # Path to your dataset root (e.g., '/content/dataset/mvtec_ad')\n",
        "    'class_name': 'bottle',      # Example: 'bottle', 'cable', etc. (must match a folder in your dataset_path)\n",
        "    'resize': [224, 224],        # Resize images to 256x256\n",
        "    'crop_size': [224, 224],     # Crop images to 224x224 after resize\n",
        "    'normalize': True,           # Enable normalization\n",
        "    'norm_mean': [0.485, 0.456, 0.406], # ImageNet means\n",
        "    'norm_std': [0.229, 0.224, 0.225],  # ImageNet stds\n",
        "    'model_data_path': './model_outputs', # Directory to save/load models (relative to AnomaVision dir)\n",
        "    'log_level': 'INFO',\n",
        "})\n",
        "\n",
        "# Create model_data_path if it doesn't exist\n",
        "os.makedirs(common_config.model_data_path, exist_ok=True)\n",
        "\n",
        "# --- Training Specific Configuration ---\n",
        "train_config = edict({\n",
        "    'backbone': 'resnet18',      # Model backbone: 'resnet18' or 'wide_resnet50'\n",
        "    'batch_size': 8,\n",
        "    'feat_dim': 50,             # Number of random feature dimensions\n",
        "    'layer_indices': [0],     # Layers to extract features from\n",
        "    'output_model': 'padim_model.pt', # Filename for the trained model\n",
        "    'run_name': 'colab_train_exp',\n",
        "    'device': \"cuda\",\n",
        "})\n",
        "\n",
        "# --- Export Specific Configuration ---\n",
        "export_config = edict({\n",
        "    'input_shape': [1, 3, 224, 224], # Input shape for ONNX/TorchScript export\n",
        "    'onnx_output_name': 'padim_model.onnx',\n",
        "    'torchscript_output_name': 'padim_model.torchscript',\n",
        "    'openvino_output_name': 'padim_model_openvino',\n",
        "    'dynamic_batch': False,\n",
        "    'quantize_dynamic_flag': False, # Set to True for dynamic INT8 quantization\n",
        "    'quantize_static_flag': False,  # Set to True for static INT8 quantization\n",
        "    'calib_samples': 100,           # Number of calibration samples for static quantization\n",
        "})\n",
        "\n",
        "# --- Detection Specific Configuration ---\n",
        "detect_config = edict({\n",
        "    # IMPORTANT: Adjust img_path to point to your test images within your dataset\n",
        "    # e.g., '/content/dataset/mvtec_ad/bottle/test'\n",
        "    'img_path': \"test_dir\", # Path to test images\n",
        "    'model': 'padim_model.onnx', # Model file to use for detection (e.g., .pt, .onnx, .torchscript)\n",
        "    'device': 'auto',            # 'auto', 'cpu', or 'cuda'\n",
        "    'batch_size': 1,\n",
        "    'thresh': 13.0,               # Anomaly classification threshold\n",
        "    'enable_visualization': False,\n",
        "    'save_visualizations': False,\n",
        "    'viz_output_dir': './visualizations',\n",
        "    'run_name': 'colab_detect_exp',\n",
        "    'overwrite': True,\n",
        "    'viz_alpha': 0.5,\n",
        "    'viz_padding': 40,\n",
        "    'viz_color': (128,0,128),\n",
        "})\n",
        "\n",
        "print(\"Configuration defined. Please review and adjust `dataset_path` and `class_name`.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import anomavision\n",
        "from anomavision.utils import (    get_logger,    setup_logging)\n",
        "\n",
        "\n",
        "  # Setup logging first\n",
        "setup_logging(enabled=True, log_level=\"INFO\", log_to_file=True)\n",
        "logger = get_logger(\"anomavision.detect\")  # Force it into anomavision hierarchy\n"
      ],
      "metadata": {
        "id": "NFXkWIraHgnv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58e09f23-8952-42a5-8604-fba8ed70e2be"
      },
      "id": "NFXkWIraHgnv",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-24 18:32:30,217 - anomavision - INFO - Anomavision logging initialized - Level: INFO\n",
            "2025-09-24 18:32:30,218 - anomavision - INFO - Log file: logs/anomavision_20250924_183230.log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6e8c0e1",
      "metadata": {
        "id": "e6e8c0e1"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2df2804",
      "metadata": {
        "id": "f2df2804"
      },
      "source": [
        "This section performs model training using the `anomavision.Padim` class directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e0ff38e7",
      "metadata": {
        "id": "e0ff38e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "686ef8ef-200b-4fbb-ade0-931a4e05c370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-24 18:32:31,773 - anomavision.feature_extraction - INFO - Initializing ResnetEmbeddingsExtractor with backbone: resnet18, device: cuda\n",
            "2025-09-24 18:32:31,774 - anomavision.feature_extraction - INFO - Loading resnet18 with weights: ResNet18_Weights.IMAGENET1K_V1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Training PaDiM Model ===\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 169MB/s]\n",
            "2025-09-24 18:32:32,477 - anomavision.feature_extraction - INFO - Backbone successfully moved to device: cuda:0\n",
            "2025-09-24 18:32:32,478 - anomavision.feature_extraction - INFO - Model set to evaluation mode\n",
            "2025-09-24 18:32:32,485 - anomavision.feature_extraction - INFO - Starting feature extraction from dataloader with 2 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backbone device: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Feature extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.75it/s]\n",
            "2025-09-24 18:32:33,223 - anomavision.feature_extraction - INFO - Feature extraction completed. Final shape: torch.Size([10, 3136, 50])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Training completed in 1.95 seconds\n",
            "Statistics saved to ./model_outputs/padim.pth using FP16 precision\n",
            "Model saved to: ./model_outputs/padim_model.pt\n",
            "Compact statistics saved to: ./model_outputs/padim.pth\n",
            "   Statistics file size: 15.26 MB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import time\n",
        "import anomavision\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from anomavision.general import Profiler, determine_device, increment_path\n",
        "from pathlib import Path\n",
        "device_str = determine_device(train_config.device)\n",
        "start_time = time.time()\n",
        "\n",
        "print(\"=== Training PaDiM Model ===\")\n",
        "\n",
        "# # Create dataset and dataloader for training\n",
        "# train_dataset = anomavision.AnodetDataset(\n",
        "#     common_config.dataset_path,\n",
        "#     resize=common_config.resize,\n",
        "#     crop_size=common_config.crop_size,\n",
        "#     normalize=common_config.normalize,\n",
        "#     mean=common_config.norm_mean,\n",
        "#     std=common_config.norm_std,\n",
        "# )\n",
        "train_dataloader = DataLoader(data, batch_size=train_config.batch_size, shuffle=False)\n",
        "\n",
        "padim_model = anomavision.Padim(\n",
        "    backbone=train_config.backbone,\n",
        "    device=device_str,\n",
        "    layer_indices=train_config.layer_indices,\n",
        "    feat_dim=train_config.feat_dim\n",
        ")\n",
        "\n",
        "padim_model.fit(train_dataloader)\n",
        "training_time = time.time() - start_time\n",
        "print(f\"‚úÖ Training completed in {training_time:.2f} seconds\")\n",
        "\n",
        "# padim_model.mean = padim_model.mean.half() if device_str.device.type != \"cpu\" else padim_model.mean\n",
        "# padim_model.cov_inv = (\n",
        "#     padim_model.cov_inv.half() if device_str.device.type != \"cpu\" else padim_model.cov_inv\n",
        "# )\n",
        "\n",
        "# Save state_dict (safer than pickling whole object)\n",
        "model_path = os.path.join(common_config.model_data_path, train_config.output_model)\n",
        "\n",
        "torch.save(padim_model, model_path)\n",
        "\n",
        "stats_path =  os.path.join(common_config.model_data_path, \"padim.pth\")\n",
        "padim_model.save_statistics(str(stats_path), half=True)\n",
        "stats_size = Path(stats_path).stat().st_size / (1024 * 1024)\n",
        "\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "print(f\"Compact statistics saved to: {stats_path}\")\n",
        "print(f\"   Statistics file size: {stats_size:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "074f480d",
      "metadata": {
        "id": "074f480d"
      },
      "source": [
        "## 4. Export"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install onnx onnxruntime-gpu\n"
      ],
      "metadata": {
        "id": "Y5zEC9bd8nvN"
      },
      "id": "Y5zEC9bd8nvN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f86e0e26",
      "metadata": {
        "id": "f86e0e26"
      },
      "source": [
        "This section exports the trained model to ONNX, TorchScript, and OpenVINO formats using the `anomavision.ModelExporter` class directly.\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4752ab43",
      "metadata": {
        "id": "4752ab43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "846b0fff-e0ae-406c-cd4b-5d71bab1505c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-24 18:32:36,885 - anomavision.detect - INFO - Auto-detected device: CUDA\n",
            "2025-09-24 18:32:36,886 - anomavision.detect - INFO - load: model_outputs/padim_model.pt\n",
            "2025-09-24 18:32:36,940 - anomavision.detect - INFO - Loaded object type: <class 'anomavision.padim.Padim'>\n",
            "2025-09-24 18:32:36,941 - anomavision.detect - INFO - GOING INTO FULL MODEL PATH\n",
            "2025-09-24 18:32:36,971 - anomavision.detect - INFO - Using FP16 precision (auto-detected for CUDA)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting export...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-24 18:32:37,754 - anomavision.detect - INFO - onnx: ok (0.87s) file=model_outputs/padim_model.onnx size=15.6MB dynamic_batch=False opset=17 precision=FP16 device=cuda\n",
            "2025-09-24 18:32:37,757 - anomavision.detect - INFO - load: model_outputs/padim_model.pt\n",
            "2025-09-24 18:32:37,816 - anomavision.detect - INFO - Loaded object type: <class 'anomavision.padim.Padim'>\n",
            "2025-09-24 18:32:37,816 - anomavision.detect - INFO - GOING INTO FULL MODEL PATH\n",
            "2025-09-24 18:32:37,840 - anomavision.detect - INFO - Using FP16 precision (auto-detected for CUDA)\n",
            "2025-09-24 18:32:37,847 - anomavision.detect - INFO - ts: tracing optimize=False precision=FP16 device=cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model exported to ONNX: model_outputs/padim_model.onnx\n",
            "   ONNX file size: 15.57 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-24 18:32:38,264 - anomavision.detect - INFO - ts: ok (0.51s) file=model_outputs/padim_model.torchscript size=37.7MB optimized=False precision=FP16 device=cuda\n",
            "2025-09-24 18:32:38,267 - anomavision.detect - INFO - OpenVINO: using FP16 (auto-detected for CUDA)\n",
            "2025-09-24 18:32:38,268 - anomavision.detect - INFO - load: model_outputs/padim_model.pt\n",
            "2025-09-24 18:32:38,316 - anomavision.detect - INFO - Loaded object type: <class 'anomavision.padim.Padim'>\n",
            "2025-09-24 18:32:38,317 - anomavision.detect - INFO - GOING INTO FULL MODEL PATH\n",
            "2025-09-24 18:32:38,346 - anomavision.detect - INFO - Using FP16 precision (forced to FP16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model exported to TorchScript: model_outputs/padim_model.torchscript\n",
            "   torchscript file size: 37.68 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-24 18:32:38,611 - anomavision.detect - INFO - onnx: ok (0.34s) file=model_outputs/temp_model.onnx size=15.6MB dynamic_batch=False opset=17 precision=FP16 device=cuda\n",
            "2025-09-24 18:32:39,649 - anomavision.detect - INFO - ov: convert fp16=True dynamic_batch=False device=cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release.\n",
            "In 2025.0 MO command line tool and openvino.tools.mo.convert_model() will be removed. Please use OpenVINO Model Converter (OVC) or openvino.convert_model(). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
            "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-09-24 18:32:40,751 - anomavision.detect - INFO - ov: ok (2.48s) dir=model_outputs/padim_model_openvino xml=padim_model_openvino.xml size=0.0MB precision=FP16 dynamic_batch=False device=cuda\n",
            "2025-09-24 18:32:40,751 - anomavision.detect - INFO - ov: ok (2.48s) dir=model_outputs/padim_model_openvino xml=padim_model_openvino.xml size=0.0MB precision=FP16 dynamic_batch=False device=cuda\n",
            "2025-09-24 18:32:40,751 - anomavision.detect - INFO - ov: ok (2.48s) dir=model_outputs/padim_model_openvino xml=padim_model_openvino.xml size=0.0MB precision=FP16 dynamic_batch=False device=cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model exported to OpenVINO: model_outputs/padim_model_openvino\n",
            "   openvino file size: 0.00 MB\n",
            "Export complete.\n",
            "‚úÖ Model exported to OpenVINO: model_outputs/padim_model_openvino‚úÖ Model exported to OpenVINO: model_outputs/padim_model_openvino"
          ]
        }
      ],
      "source": [
        "\n",
        "import anomavision\n",
        "from export import ModelExporter\n",
        "\n",
        "model_path = os.path.join(common_config.model_data_path, train_config.output_model)\n",
        "output_dir = common_config.model_data_path\n",
        "\n",
        "# # Assuming a logger is not strictly necessary for basic Colab usage, or can be simplified\n",
        "# class DummyLogger:\n",
        "#     def info(self, *args): print(\"INFO:\", *args)\n",
        "#     def exception(self, *args): print(\"EXCEPTION:\", *args)\n",
        "#     def warning(self, *args): print(\"WARNING:\", *args)\n",
        "# logger = DummyLogger()\n",
        "\n",
        "exporter = ModelExporter(model_path, output_dir, logger,device=\"cuda\")\n",
        "\n",
        "print(\"Starting export...\")\n",
        "\n",
        "# Export to ONNX\n",
        "onnx_path = exporter.export_onnx(\n",
        "    input_shape=export_config.input_shape,\n",
        "    output_name=export_config.onnx_output_name,\n",
        "    dynamic_batch=export_config.dynamic_batch,\n",
        "    quantize_dynamic_flag=export_config.quantize_dynamic_flag,\n",
        "    quantize_static_flag=export_config.quantize_static_flag,\n",
        "    calib_samples=export_config.calib_samples,\n",
        "    calib_dir=common_config.dataset_path,\n",
        "    # force_precision=\"fp32\" if export_config.quantize_static_flag else None,\n",
        ")\n",
        "\n",
        "if onnx_path:\n",
        "    print(f\"‚úÖ Model exported to ONNX: {onnx_path}\")\n",
        "    stats_size = onnx_path.stat().st_size / (1024 * 1024)\n",
        "    print(f\"   ONNX file size: {stats_size:.2f} MB\")\n",
        "\n",
        "\n",
        "# Export to TorchScript\n",
        "torchscript_path = exporter.export_torchscript(\n",
        "    input_shape=export_config.input_shape,\n",
        "    output_name=export_config.torchscript_output_name,\n",
        ")\n",
        "if torchscript_path:\n",
        "    print(f\"‚úÖ Model exported to TorchScript: {torchscript_path}\")\n",
        "    stats_size = torchscript_path.stat().st_size / (1024 * 1024)\n",
        "    print(f\"   torchscript file size: {stats_size:.2f} MB\")\n",
        "\n",
        "\n",
        "# Export to OpenVINO (requires ONNX as intermediate)\n",
        "openvino_path = exporter.export_openvino(\n",
        "    input_shape=export_config.input_shape,\n",
        "    output_name=export_config.openvino_output_name,\n",
        "    dynamic_batch=export_config.dynamic_batch,\n",
        ")\n",
        "if openvino_path:\n",
        "    print(f\"‚úÖ Model exported to OpenVINO: {openvino_path}\")\n",
        "    stats_size = openvino_path.stat().st_size / (1024 * 1024)\n",
        "    print(f\"   openvino file size: {stats_size:.2f} MB\")\n",
        "\n",
        "\n",
        "print(\"Export complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[link text](https://)## 4.1. TensorRT"
      ],
      "metadata": {
        "id": "6KNSXzXyaD5z"
      },
      "id": "6KNSXzXyaD5z"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch-tensorrt\n",
        "\n",
        "# !pip install nvidia-pyindex nvidia-pip\n",
        "# !pip install nvidia-tensorrt==8.6.1\n",
        "!pip install torch-tensorrt==2.8.0\n",
        "\n"
      ],
      "metadata": {
        "id": "ERYywr-1aDfG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c345f2c3-2eb0-4987-b427-b3044cae087c"
      },
      "id": "ERYywr-1aDfG",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-tensorrt==2.8.0\n",
            "  Downloading torch_tensorrt-2.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_34_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=23 in /usr/local/lib/python3.12/dist-packages (from torch-tensorrt==2.8.0) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from torch-tensorrt==2.8.0) (4.15.0)\n",
            "Collecting dllist (from torch-tensorrt==2.8.0)\n",
            "  Downloading dllist-2.0.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: torch<2.9.0,>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-tensorrt==2.8.0) (2.8.0)\n",
            "Collecting tensorrt<10.13.0,>=10.12.0 (from torch-tensorrt==2.8.0)\n",
            "  Downloading tensorrt-10.12.0.36.tar.gz (40 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt-cu12-bindings<10.13.0,>=10.12.0 (from torch-tensorrt==2.8.0)\n",
            "  Downloading tensorrt_cu12_bindings-10.12.0.36-cp312-none-manylinux_2_28_x86_64.whl.metadata (607 bytes)\n",
            "Collecting tensorrt-cu12-libs<10.13.0,>=10.12.0 (from torch-tensorrt==2.8.0)\n",
            "  Downloading tensorrt_cu12_libs-10.12.0.36.tar.gz (709 bytes)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-tensorrt==2.8.0) (1.26.4)\n",
            "Collecting tensorrt_cu12==10.12.0.36 (from tensorrt<10.13.0,>=10.12.0->torch-tensorrt==2.8.0)\n",
            "  Downloading tensorrt_cu12-10.12.0.36.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12 in /usr/local/lib/python3.12/dist-packages (from tensorrt-cu12-libs<10.13.0,>=10.12.0->torch-tensorrt==2.8.0) (12.8.90)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (2.8.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (2025.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<2.9.0,>=2.8.0->torch-tensorrt==2.8.0) (3.0.2)\n",
            "Downloading torch_tensorrt-2.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_34_x86_64.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m124.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorrt_cu12_bindings-10.12.0.36-cp312-none-manylinux_2_28_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dllist-2.0.0-py3-none-any.whl (5.7 kB)\n",
            "Building wheels for collected packages: tensorrt, tensorrt_cu12, tensorrt-cu12-libs\n",
            "\u001b[33m  DEPRECATION: Building 'tensorrt' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'tensorrt'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt: filename=tensorrt-10.12.0.36-py2.py3-none-any.whl size=46733 sha256=c51ce9592a9abaf27994601b54d91183e6c3f2b8f458f63723d80ca7a232b8d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/d5/2f/cc5e3e56d49c61a02a7a8313f37db27d9af00e7f3463ed33e7\n",
            "\u001b[33m  DEPRECATION: Building 'tensorrt_cu12' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'tensorrt_cu12'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for tensorrt_cu12 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt_cu12: filename=tensorrt_cu12-10.12.0.36-py2.py3-none-any.whl size=17584 sha256=1dc57410414e10c12699fa2bfc53f5a9eb0c813c7530c14c82eaca049944668c\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/f7/e4/64a0965dcc74c067cee07482ef03cc18add0def626fb0ebc1c\n",
            "  Building wheel for tensorrt-cu12-libs (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt-cu12-libs: filename=tensorrt_cu12_libs-10.12.0.36-py2.py3-none-manylinux_2_28_x86_64.whl size=3095483544 sha256=3910039e1d49de0edfdc8bf273e40ad4b85a9d57c7c383fe0e22f75417df9610\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/8f/cb/35c0a5f8d17ece1927e42f2c49108ec506fd79b8e00490d416\n",
            "Successfully built tensorrt tensorrt_cu12 tensorrt-cu12-libs\n",
            "Installing collected packages: tensorrt-cu12-bindings, tensorrt-cu12-libs, dllist, tensorrt_cu12, tensorrt, torch-tensorrt\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6/6\u001b[0m [torch-tensorrt]\n",
            "\u001b[1A\u001b[2KSuccessfully installed dllist-2.0.0 tensorrt-10.12.0.36 tensorrt-cu12-bindings-10.12.0.36 tensorrt-cu12-libs-10.12.0.36 tensorrt_cu12-10.12.0.36 torch-tensorrt-2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accelerating the model with TensorRT**\n"
      ],
      "metadata": {
        "id": "Hmk5H0xEabeM"
      },
      "id": "Hmk5H0xEabeM"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VfXIHTpifc-_"
      },
      "id": "VfXIHTpifc-_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# re-trace\n",
        "example_input = torch.randn(1, 3, 224, 224).cuda()\n",
        "ts_model = torch.jit.trace(padim_model.cuda().eval(), example_input)\n",
        "ts_model.save(\"padim_fixed.ts\")\n",
        "\n"
      ],
      "metadata": {
        "id": "gR6fcCQFaheA"
      },
      "id": "gR6fcCQFaheA",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip list"
      ],
      "metadata": {
        "id": "FZPW4gig2XBs"
      },
      "id": "FZPW4gig2XBs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts_model = torch.jit.load(\"padim_fixed.ts\").eval().cuda()\n",
        "ts_model = ts_model.half()             # convert weights & ops to half\n",
        "example_input = torch.randn(1, 3, 224, 224, device=\"cuda\", dtype=torch.float16)\n",
        "torch.jit.save(torch.jit.trace(ts_model, example_input), \"padim_fp16.ts\")\n"
      ],
      "metadata": {
        "id": "DyIeNGFZEwm1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c358b88f-287b-45ab-fb51-cc7a65b11538"
      },
      "id": "DyIeNGFZEwm1",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/jit/_trace.py:685: UserWarning: The input to trace is already a ScriptModule, tracing it is a no-op. Returning the object as is.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch_tensorrt as torchtrt\n",
        "\n",
        "\n",
        "\n",
        "trt_model = torchtrt.compile(\n",
        "    torch.jit.load(\"padim_fp16.ts\").eval().cuda(),\n",
        "    ir=\"torchscript\",\n",
        "    inputs=[torchtrt.Input(example_input.shape, dtype=torch.float16)],\n",
        "    enabled_precisions={torch.float16},\n",
        "    truncate_long_and_double=True,\n",
        "    require_full_compilation=False,\n",
        ")\n",
        "torch.jit.save(trt_model, \"padim_trt.ts\")"
      ],
      "metadata": {
        "id": "AMX6bosZcui7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "629bb4ff-aed3-4fb0-c3d1-850f849d1ff7"
      },
      "id": "AMX6bosZcui7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torch_tensorrt.dynamo.conversion.aten_ops_converters:Unable to import quantization op. Please install modelopt library (https://github.com/NVIDIA/TensorRT-Model-Optimizer?tab=readme-ov-file#installation) to add support for compiling quantized models\n",
            "WARNING:torch_tensorrt.dynamo.conversion.aten_ops_converters:Unable to import quantize op. Please install modelopt library (https://github.com/NVIDIA/TensorRT-Model-Optimizer?tab=readme-ov-file#installation) to add support for compiling quantized models\n",
            "WARNING:torch_tensorrt.dynamo.conversion.converter_utils:TensorRT-LLM is not installed. Please install TensorRT-LLM or set TRTLLM_PLUGINS_PATH to the directory containing libnvinfer_plugin_tensorrt_llm.so to use converters for torch.distributed ops\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 1) Load compiled TRT module\n",
        "trt_model = torch.jit.load(\"padim_trt.ts\").eval().cuda()\n",
        "\n",
        "# 2) Create half-precision input\n",
        "x = torch.randn(1, 3, 224, 224, device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "# 3) Inference\n",
        "with torch.no_grad():\n",
        "    scores, embeddings = trt_model(x)   # returns tuple (Tensor, Tensor)\n",
        "\n",
        "print(\"Scores shape:\", scores.shape, \"dtype:\", scores.dtype)\n",
        "print(\"Embeddings shape:\", embeddings.shape, \"dtype:\", embeddings.dtype)\n"
      ],
      "metadata": {
        "id": "hCYa8rXzjUH2"
      },
      "id": "hCYa8rXzjUH2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "torch.cuda.synchronize()\n",
        "for _ in range(10):\n",
        "    t0 = time.time()\n",
        "    x = torch.randn(1, 3, 224, 224, device=\"cuda\", dtype=torch.float16)\n",
        "    s,m = trt_model(x)\n",
        "    # print(f\"============ {s.dtype}, {m.dtype}\")\n",
        "    t1 = time.time()\n",
        "    print(f\"Total pipeline time: {(t1 - t0) * 1000:.2f} ms\")\n",
        "\n",
        "torch.cuda.synchronize()\n"
      ],
      "metadata": {
        "id": "5ftoqGuKTDK9"
      },
      "id": "5ftoqGuKTDK9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(trt_model)  # sometimes shows forward signature\n",
        "try:\n",
        "    print(trt_model.graph)   # TorchScript graph, look for 'execute_engine([..., ...])'\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Check parameters / buffers that could be fp32\n",
        "for n, b in trt_model.named_buffers(recurse=True):\n",
        "    if b.dtype == torch.float32:\n",
        "        print(\"FP32 buffer:\", n, b.shape)\n",
        "\n",
        "for n, p in trt_model.named_parameters(recurse=True):\n",
        "    if p.dtype == torch.float32:\n",
        "        print(\"FP32 param:\", n, p.shape)\n"
      ],
      "metadata": {
        "id": "wtflAHOYOn7D"
      },
      "id": "wtflAHOYOn7D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from anomavision.inference.model.wrapper import ModelWrapper\n",
        "import torch\n",
        "\n",
        "device_str = \"cuda\"\n",
        "\n",
        "wrapper = ModelWrapper(torchscript_path, device_str)\n",
        "\n",
        "# Access the actual TorchScript model\n",
        "ts_model = wrapper.backend.model\n",
        "# Access TorchScript model directly\n",
        "ts_model = wrapper.backend.model\n",
        "ts_model.eval().cuda()\n",
        "\n",
        "\n",
        "dtypes = {p.dtype for p in ts_model.parameters()} | {b.dtype for b in ts_model.buffers()}\n",
        "print(\"Detected dtypes in model:\", dtypes)\n",
        "\n",
        "\n",
        "\n",
        "# Save it again (if you want a cleaned copy)\n",
        "ts_model.save(\"padim_fixed.ts\")\n",
        "\n",
        "dtypes = {p.dtype for p in ts_model.parameters()} | {b.dtype for b in ts_model.buffers()}\n",
        "\n",
        "print(\"Detected dtypes in model:\", dtypes)\n",
        "\n",
        "if torch.float16 in dtypes:\n",
        "    print(\"‚úÖ Model contains FP16 weights\")\n",
        "if torch.float32 in dtypes:\n",
        "    print(\"‚úÖ Model contains FP32 weights\")\n",
        "\n",
        "\n",
        "# Convert to FP16\n",
        "ts_model = ts_model.half()\n",
        "\n",
        "# Save a new TorchScript in FP16\n",
        "example_input = torch.randn(1, 3, 224, 224, device=\"cuda\", dtype=torch.float16)\n",
        "ts_model_fp16 = torch.jit.trace(ts_model, example_input)\n",
        "ts_model_fp16.save(\"padim_fp16.ts\")\n",
        "\n",
        "# Double-check\n",
        "dtypes = {p.dtype for p in ts_model_fp16.parameters()} | {b.dtype for b in ts_model_fp16.buffers()}\n",
        "print(\"Detected dtypes after conversion:\", dtypes)\n"
      ],
      "metadata": {
        "id": "7sjh76jOm59_"
      },
      "id": "7sjh76jOm59_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb\n",
        "pdb.set_trace()\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch\n",
        "import torch_tensorrt as torchtrt\n",
        "\n",
        "\n",
        "\n",
        "trt_model = torchtrt.compile(\n",
        "    torch.jit.load(\"padim_fp16.ts\").eval().cuda(),\n",
        "    ir=\"torchscript\",\n",
        "    inputs=[torchtrt.Input(example_input.shape, dtype=torch.float16)],\n",
        "    enabled_precisions={torch.float16},\n",
        "    truncate_long_and_double=True,\n",
        "    require_full_compilation=False,\n",
        ")\n",
        "torch.jit.save(trt_model, \"padim_trt.ts\")\n",
        "# 1) Load compiled TRT module\n",
        "trt_model = torch.jit.load(\"padim_trt.ts\").eval().cuda()\n",
        "\n",
        "# 2) Create half-precision input\n",
        "x = torch.randn(1, 3, 224, 224, device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "# 3) Inference\n",
        "with torch.no_grad():\n",
        "    scores, embeddings = trt_model(x)   # returns tuple (Tensor, Tensor)\n",
        "\n",
        "print(\"Scores shape:\", scores.shape, \"dtype:\", scores.dtype)\n",
        "print(\"Embeddings shape:\", embeddings.shape, \"dtype:\", embeddings.dtype)\n"
      ],
      "metadata": {
        "id": "-hfJkewEAJ5D"
      },
      "id": "-hfJkewEAJ5D",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}